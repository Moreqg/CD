name: base
version: f0
fold: &fold 0
# folds is 5, which is used to split train and valid datasets

model:
  type: MMSegModel
  backbone:
    type: timm
    model_name: efficientnet_b0
    pretrained: True
    in_chans: 6
  decode_head:
    type: UPerHead
    pool_scales: [1, 2, 3, 6]
    channels: 512
    dropout_ratio: 0.1
    num_classes: 1
    norm_cfg: {type: BN, requires_grad: True}
    align_corners: False
#model:
#  type: SMPModel
#  model_type: Unet
#  model_name: timm-efficientnet-b2
#  pretrained_weight: noisy-student
#  num_classes: 1
loss: [{
  type: BCEWithIgnoreLoss, 
  loss_name: bce_loss, 
  ignore_index: 255, 
  loss_weight: 1.0,
}, {
  type: DiceLoss, 
  loss_name: dice_loss, 
  loss_weight: 1.0, 
}]

metric: 
  type: DiceMetric

data:
  type: ConcatData
  fold: 0
  num_folds: 5
  batch_size: 8
  stratified_by: null
  group_by: null
  dataset:
    resize: &resize 512
    trans: {
      train: {type: Compose, transforms: [
        {type: Resize, height: *resize, width: *resize},
        {type: HorizontalFlip, p: 0.5},
        {type: VerticalFlip, p: 0.5},
        {type: Normalize},
        {type: ToTensorV2},
      ], additional_targets: {imageB: image}},
      val: {type: Compose, transforms: [
        {type: Resize, height: *resize, width: *resize},
        {type: Normalize},
        {type: ToTensorV2},
      ], additional_targets: {imageB: image}}
    }

train:
  # optimizer
  optimizer: adam
  learning_rate: 1e-3
  weight_decay: 2e-5

  # scheduler
  num_epochs: 130
  scheduler: one_cycle

  # trainer
  monitor: val_dice
  log_step: 50
  val_interval: 1
  swa: False
  grad_clip: 2.0
  strategy: dp